{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a6198e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/demucs/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import hydra\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "import torchaudio as ta\n",
    "import musdb\n",
    "\n",
    "#library for class Wavset\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch as th\n",
    "import julius\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#library for loader()\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca78f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# musdb_p = '/workspace/MusicDataset/musdb18hq'\n",
    "musdb_samplerate= 44100\n",
    "# wav=  # path to custom wav dataset\n",
    "segment= 11\n",
    "shift= 1\n",
    "# train_valid= False\n",
    "# full_cv= True\n",
    "samplerate= 44100\n",
    "channels= 2\n",
    "normalize= True\n",
    "metadata= './metadata'\n",
    "sources= ['drums', 'bass', 'other', 'vocals']\n",
    "EXT = \".wav\"\n",
    "MIXTURE = \"mixture\"\n",
    "download = False\n",
    "\n",
    "batch_size= 6\n",
    "num_workers = 10\n",
    "world_size = 1\n",
    "root= '/workspace/MusicDataset/musdb18hq'\n",
    "# root= '/workspace/helen/demucs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca549fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_musdb_valid():\n",
    "    # Return musdb valid set.\n",
    "    import yaml\n",
    "    setup_path = Path(musdb.__path__[0]) / 'configs' / 'mus.yaml'\n",
    "    setup = yaml.safe_load(open(setup_path, 'r'))\n",
    "    return setup['validation_tracks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93ddfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _track_metadata(track, sources, normalize=True, ext=EXT):\n",
    "    track_length = None\n",
    "    track_samplerate = None\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    for source in sources + [MIXTURE]:\n",
    "        file = track / f\"{source}{ext}\"\n",
    "        try:\n",
    "            info = ta.info(str(file))\n",
    "        except RuntimeError:\n",
    "            print(file)\n",
    "            raise\n",
    "        length = info.num_frames\n",
    "        if track_length is None:\n",
    "            track_length = length\n",
    "            track_samplerate = info.sample_rate\n",
    "        elif track_length != length:\n",
    "            raise ValueError(\n",
    "                f\"Invalid length for file {file}: \"\n",
    "                f\"expecting {track_length} but got {length}.\")\n",
    "        elif info.sample_rate != track_samplerate:\n",
    "            raise ValueError(\n",
    "                f\"Invalid sample rate for file {file}: \"\n",
    "                f\"expecting {track_samplerate} but got {info.sample_rate}.\")\n",
    "        if source == MIXTURE and normalize:\n",
    "            try:\n",
    "                wav, _ = ta.load(str(file))\n",
    "            except RuntimeError:\n",
    "                print(file)\n",
    "                raise\n",
    "            wav = wav.mean(0)\n",
    "            mean = wav.mean().item()\n",
    "            std = wav.std().item()\n",
    "\n",
    "    return {\"length\": length, \"mean\": mean, \"std\": std, \"samplerate\": track_samplerate}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2b7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata(path, sources, normalize=True, ext=EXT):\n",
    "    \"\"\"\n",
    "    Build the metadata for `Wavset`.\n",
    "    Args:\n",
    "        path (str or Path): path to dataset.\n",
    "        sources (list[str]): list of sources to look for.\n",
    "        normalize (bool): if True, loads full track and store normalization\n",
    "            values based on the mixture file.\n",
    "        ext (str): extension of audio files (default is .wav).\n",
    "    \"\"\"\n",
    "\n",
    "    meta = {}\n",
    "    path = Path(path)\n",
    "    pendings = []\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(8) as pool:\n",
    "        for root, folders, files in os.walk(path, followlinks=True):\n",
    "            root = Path(root)\n",
    "            if root.name.startswith('.') or folders or root == path:\n",
    "                continue\n",
    "            name = str(root.relative_to(path))\n",
    "            pendings.append((name, pool.submit(_track_metadata, root, sources, normalize, ext)))\n",
    "            # meta[name] = _track_metadata(root, sources, normalize, ext)\n",
    "        for name, pending in tqdm.tqdm(pendings, ncols=120):\n",
    "            meta[name] = pending.result()\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04724232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_channels(wav, channels=2):\n",
    "    \"\"\"Convert audio to the given number of channels.\"\"\"\n",
    "    *shape, src_channels, length = wav.shape\n",
    "    if src_channels == channels:\n",
    "        pass\n",
    "    elif channels == 1:\n",
    "        # Case 1:\n",
    "        # The caller asked 1-channel audio, but the stream have multiple\n",
    "        # channels, downmix all channels.\n",
    "        wav = wav.mean(dim=-2, keepdim=True)\n",
    "    elif src_channels == 1:\n",
    "        # Case 2:\n",
    "        # The caller asked for multiple channels, but the input file have\n",
    "        # one single channel, replicate the audio over all channels.\n",
    "        wav = wav.expand(*shape, channels, length)\n",
    "    elif src_channels >= channels:\n",
    "        # Case 3:\n",
    "        # The caller asked for multiple channels, and the input file have\n",
    "        # more channels than requested. In that case return the first channels.\n",
    "        wav = wav[..., :channels, :]\n",
    "    else:\n",
    "        # Case 4: What is a reasonable choice here?\n",
    "        raise ValueError('The audio file has less channels than requested but is not mono.')\n",
    "    return wav\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a30d2",
   "metadata": {},
   "source": [
    "metadata_train: 86 songs\\\n",
    "metadata_valid: 14 songs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df349f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# wget.download('https://zenodo.org/record/3338373/files/musdb18hq.zip?download=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47e68e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusdbHQ:\n",
    "    def __init__(\n",
    "            self,root, subset,segment=None, shift=None, normalize=True,\n",
    "            samplerate=44100, channels=2, ext=EXT):\n",
    "        \"\"\"\n",
    "        Waveset (or mp3 set for that matter). Can be used to train\n",
    "        with arbitrary sources. Each track should be one folder inside of `path`.\n",
    "        The folder should contain files named `{source}.{ext}`.\n",
    "        Args:\n",
    "            root (Path or str): root folder for the dataset.\n",
    "            subset (str): training or validation\n",
    "            download (bool): Whether to download the dataset if it is not found at root path. (default: ``False``).\n",
    "            segment (None or float): segment length in seconds. If `None`, returns entire tracks.\n",
    "            shift (None or float): stride in seconds bewteen samples.\n",
    "            normalize (bool): normalizes input audio, **based on the metadata content**,\n",
    "                i.e. the entire track is normalized, not individual extracts.\n",
    "            samplerate (int): target sample rate. if the file sample rate\n",
    "                is different, it will be resampled on the fly.\n",
    "            channels (int): target nb of channels. if different, will be\n",
    "                changed onthe fly.\n",
    "            ext (str): extension for audio files (default is .wav).\n",
    "        samplerate and channels are converted on the fly.\n",
    "        \"\"\"\n",
    "#         url = 'https://zenodo.org/record/3338373/files/musdb18hq.zip?download=1'\n",
    "        \n",
    "#         download_path = root\n",
    "#         self.download_path = download_path\n",
    "        \n",
    "          \n",
    "#         if download:\n",
    "#             if os.path.isdir(download_path) and os.path.isdir(os.path.join(download_path, 'data')):\n",
    "#                 print(f'Dataset folder exists, skipping download...')\n",
    "#                 decision = input(f\"Do you want to extract {archive_name+ext_archive} again? \"\n",
    "#                                  f\"To avoid this prompt, set `download=False`\\n\"\n",
    "#                                  f\"This action will overwrite exsiting files, do you still want to continue? [yes/no]\") \n",
    "#                 if decision.lower()=='yes':\n",
    "#                     print(f'extracting...')\n",
    "#                     extract_archive(os.path.join(download_path, archive_name+ext_archive))                \n",
    "#             elif os.path.isfile(os.path.join(download_path, 'timit.zip')):\n",
    "#                 print(f'timit.zip exists, extracting...')\n",
    "#                 check_md5(os.path.join(download_path, archive_name+ext_archive), checksum)\n",
    "#                 extract_archive(os.path.join(download_path, archive_name+ext_archive))\n",
    "#             else:\n",
    "#                 decision='yes'       \n",
    "#                 if not os.path.isdir(download_path):\n",
    "#                     print(f'Creating download path = {root}')\n",
    "#                     os.makedirs(os.path.join(download_path))\n",
    "# #                 if os.path.isfile(download_path+ext_archive):\n",
    "# #                     print(f'.tar.gz file exists, proceed to extraction...')\n",
    "# #                 else:\n",
    "#                 if os.path.isfile(os.path.join(download_path, archive_name+ext_archive)):\n",
    "#                     print(f'{download_path+ext_archive} already exists, proceed to extraction...')\n",
    "#                 else:\n",
    "#                     print(f'downloading...')\n",
    "#                     try:\n",
    "#                         download_url(url, download_path, hash_value=checksum, hash_type='md5')\n",
    "#                     except:\n",
    "#                         raise Exception('Auto download fails. '+\n",
    "#                                         'You may want to download it manually from:\\n'+\n",
    "#                                         url+ '\\n' +\n",
    "#                                         f'Then, put it inside {download_path}')\n",
    "                      \n",
    "        \n",
    "        \n",
    "# get_musdb_wav_datasets\n",
    "        sig = hashlib.sha1(str(root).encode()).hexdigest()[:8]\n",
    "        metadata_file = Path('./metadata') / ('musdb_' + sig + \".json\")\n",
    "        root = Path(root) / \"train\"\n",
    "    #     if not metadata_file.is_file() and distrib.rank == 0:\n",
    "        if not metadata_file.is_file():\n",
    "            metadata_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "            metadata = build_metadata(root, sources)\n",
    "            json.dump(metadata, open(metadata_file, \"w\"))\n",
    "    #     if distrib.world_size > 1:\n",
    "    #         distributed.barrier()\n",
    "        metadata = json.load(open(metadata_file))\n",
    "\n",
    "        valid_tracks = _get_musdb_valid()\n",
    "        \n",
    "        if subset == 'training':\n",
    "            metadata = {name: meta for name, meta in metadata.items() if name not in valid_tracks}\n",
    "            self.sources = sources\n",
    "            \n",
    "        elif subset=='validation':\n",
    "            metadata = {name: meta for name, meta in metadata.items() if name in valid_tracks}\n",
    "            self.sources = [MIXTURE] + list(sources)\n",
    "# metadata (dict): output from `build_metadata`.\n",
    "# sources (list[str]): list of source names.\n",
    "        \n",
    "        self.root = Path(root)\n",
    "        self.metadata = OrderedDict(metadata)\n",
    "        self.segment = segment\n",
    "        self.shift = shift or segment\n",
    "        self.normalize = normalize\n",
    "        self.channels = channels\n",
    "        self.samplerate = samplerate\n",
    "        self.ext = ext\n",
    "        self.num_examples = []\n",
    "        for name, meta in self.metadata.items():\n",
    "            track_duration = meta['length'] / meta['samplerate']\n",
    "            if segment is None or track_duration < segment:\n",
    "                examples = 1\n",
    "            else:\n",
    "                examples = int(math.ceil((track_duration - self.segment) / self.shift) + 1)\n",
    "            self.num_examples.append(examples)\n",
    "# samplerate = number of sample per second\n",
    "# length = number of sample\n",
    "# track_duration is second, cut song into segment\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_examples)\n",
    "\n",
    "    def get_file(self, name, source):\n",
    "        return self.root / name / f\"{source}{self.ext}\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(len(self.metadata))\n",
    "        for name, examples in zip(self.metadata, self.num_examples):           \n",
    "            if index >= examples:\n",
    "                index -= examples\n",
    "                continue\n",
    "            meta = self.metadata[name]\n",
    "            num_frames = -1\n",
    "            offset = 0\n",
    "            if self.segment is not None:\n",
    "                offset = int(meta['samplerate'] * self.shift * index)\n",
    "                num_frames = int(math.ceil(meta['samplerate'] * self.segment))\n",
    "            wavs = []\n",
    "            for source in self.sources:\n",
    "                file = self.get_file(name, source)\n",
    "                wav, _ = ta.load(str(file), frame_offset=offset, num_frames=num_frames)\n",
    "                wav = convert_audio_channels(wav, self.channels)\n",
    "                wavs.append(wav)\n",
    "\n",
    "            example = th.stack(wavs)\n",
    "            example = julius.resample_frac(example, meta['samplerate'], self.samplerate)\n",
    "            if self.normalize:\n",
    "                example = (example - meta['mean']) / meta['std']\n",
    "            if self.segment:\n",
    "                length = int(self.segment * self.samplerate)\n",
    "                example = example[..., :length]\n",
    "                example = F.pad(example, (0, length - example.shape[-1]))\n",
    "            return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b4732f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.07it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set = MusdbHQ(root,'training',samplerate=samplerate, channels=channels,normalize=normalize,\n",
    "                    segment=segment, shift=shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c974c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = MusdbHQ(root,'validation', samplerate=samplerate, channels=channels,normalize=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12a8fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 485100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ffc0cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 11301609])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(dataset, batch_size , shuffle=False, klass=DataLoader, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a dataloader properly in case of distributed training.\n",
    "    If a gradient is going to be computed you must set `shuffle=True`.\n",
    "    \"\"\"\n",
    "    if world_size == 1:\n",
    "        return klass(dataset, batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "\n",
    "    if shuffle:\n",
    "        # train means we will compute backward, we use DistributedSampler\n",
    "        sampler = DistributedSampler(dataset)\n",
    "        # We ignore shuffle, DistributedSampler already shuffles\n",
    "        return klass(dataset, batch_size=batch_size, **kwargs, sampler=sampler)\n",
    "    else:\n",
    "        # We make a manual shard, as DistributedSampler otherwise replicate some examples\n",
    "        dataset = Subset(dataset, list(range(rank, len(dataset), world_size)))\n",
    "        return klass(dataset, batch_size=batch_size, shuffle=shuffle, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = loader(\n",
    "        train_set, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:demucs] *",
   "language": "python",
   "name": "conda-env-demucs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
